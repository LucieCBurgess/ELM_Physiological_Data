Last login: Tue Aug  8 14:52:59 on console
DIGI-LBurgess-6:~ lucieburgess$ cd /Applications/spark-2.2.0-bin-hadoop2.7/examples/src/main/resources 
DIGI-LBurgess-6:resources lucieburgess$ cd /Applications/spark-2.2.0-bin-hadoop2.7 
DIGI-LBurgess-6:spark-2.2.0-bin-hadoop2.7 lucieburgess$ ./bin/spark
-bash: ./bin/spark: No such file or directory
DIGI-LBurgess-6:spark-2.2.0-bin-hadoop2.7 lucieburgess$ ./bin/spark-shell
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/08/08 15:33:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/08 15:34:02 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Spark context Web UI available at http://192.168.1.163:4040
Spark context available as 'sc' (master = local[*], app id = local-1502202834644).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.2.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112)
Type in expressions to have them evaluated.
Type :help for more information.

scala> case class Person(name: String, age: Int)
defined class Person

scala> 

scala> val df = spark.sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(attributes=> Person(attributes(0),attributes(1).trim.toInt)).toDF()
<console>:25: error: value sc is not a member of org.apache.spark.sql.SparkSession
       val df = spark.sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(attributes=> Person(attributes(0),attributes(1).trim.toInt)).toDF()
                      ^

scala> val df = spark.sparkContext.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(attributes=> Person(attributes(0),attributes(1).trim.toInt)).toDF()
df: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> df.count()
res0: Long = 3

scala> df.head()
res1: org.apache.spark.sql.Row = [Michael,29]

scala> df.show()
+-------+---+
|   name|age|
+-------+---+
|Michael| 29|
|   Andy| 30|
| Justin| 19|
+-------+---+


scala> df.take(1)
res3: Array[org.apache.spark.sql.Row] = Array([Michael,29])

scala> df.show(2,0)
+-------+---+
|name   |age|
+-------+---+
|Michael|29 |
|Andy   |30 |
+-------+---+
only showing top 2 rows


scala> df.show(2,1)
+----+---+
|name|age|
+----+---+
|   M|  2|
|   A|  3|
+----+---+
only showing top 2 rows


scala> df.show(2,5)
+-----+---+
| name|age|
+-----+---+
|Mi...| 29|
| Andy| 30|
+-----+---+
only showing top 2 rows


scala> peopleDF.createOrReplaceTempView("people")
<console>:24: error: not found: value peopleDF
       peopleDF.createOrReplaceTempView("people")
       ^

scala> df.createOrReplaceTempView("people")

scala> val teenagers.DF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")
<console>:23: error: not found: value teenagers
       val teenagers.DF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")
           ^

scala> val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")
teenagersDF: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> teenagersDF.show()
+------+---+
|  name|age|
+------+---+
|Justin| 19|
+------+---+


scala> df.take(1)(0)
res10: org.apache.spark.sql.Row = [Michael,29]

scala> df.take(1)(_.1)
<console>:1: error: ')' expected but double literal found.
df.take(1)(_.1)
            ^

scala> df.take(1)(_1)
<console>:28: error: not found: value _1
       df.take(1)(_1)
                  ^

scala> val myArray = df.take(1)
myArray: Array[org.apache.spark.sql.Row] = Array([Michael,29])

scala> myArray(0)
res12: org.apache.spark.sql.Row = [Michael,29]

scala> myArray(1)
java.lang.ArrayIndexOutOfBoundsException: 1
  ... 48 elided

scala> myArray(._1)
<console>:1: error: illegal start of simple expression
myArray(._1)
        ^

scala> println(myArray._1)
<console>:30: error: value _1 is not a member of Array[org.apache.spark.sql.Row]
       println(myArray._1)
                       ^

scala> teenagersDF.map(teenager => teenager(0)).show()
<console>:26: error: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.
       teenagersDF.map(teenager => teenager(0)).show()
                      ^

scala> import spark.implicits._
import spark.implicits._

scala> teenagersDF.map(teenager => teenager(0)).show()
<console>:29: error: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.
       teenagersDF.map(teenager => teenager(0)).show()
                      ^

scala> teenagersDF
res17: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> teenagersDF.map(teenager => "Name: " + teenager(0)).show()
+------------+
|       value|
+------------+
|Name: Justin|
+------------+


scala> teenagersDF.map(teenager => teenager(0))
<console>:29: error: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.
       teenagersDF.map(teenager => teenager(0))
                      ^

scala> teenagersDF.map(teenager => ""+ teenager(0)).show()
+------+
| value|
+------+
|Justin|
+------+


scala> teenagersDF.map(teenager => teenager.getAs[String]("name")).show()
+------+
| value|
+------+
|Justin|
+------+


scala> 
